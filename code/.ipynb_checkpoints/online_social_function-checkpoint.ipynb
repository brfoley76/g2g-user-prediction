{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "from textstat.textstat import textstat\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "import json\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The G2GAnalysis class holds all the data. It has class methods for scraping new data, keeping track of what it has already scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.wikitree.com/g2g//tags?start=0\n",
      "('irish_roots', 534)\n"
     ]
    }
   ],
   "source": [
    "class G2GAnalysis():\n",
    "    def __init__(self):\n",
    "        self.home='https://www.wikitree.com/g2g/tags'\n",
    "        self.path=self.home[:(self.home.find('tags'))]\n",
    "            \n",
    "        self.contribURL='https://www.wikitree.com/index.php?title=Special:Contributions&who='\n",
    "\n",
    "        self.tagPageIndex=0  #which page of tags am I on?\n",
    "        self.TagList=[]\n",
    "        self.Users={} #dict holding the users\n",
    "        self.Questions={} # dict holding the questions\n",
    "        self.flipped=0 # number of questions flipped through\n",
    "        self.page_tick=0\n",
    "            \n",
    "            \n",
    "    def runScrape(self, nQuestions=99999):\n",
    "        '''\n",
    "        Run through a bunch of pages of g2g questions\n",
    "        Parse them for the users, the users comments\n",
    "        and the user stats.\n",
    "        Fill a dictionary of results.\n",
    "        '''\n",
    "        self.nQuestions=nQuestions        \n",
    "        self.topQuestion='none' # keep track of what question is on top of each page\n",
    "                                # if it doesn't change after you try to change a page\n",
    "                                # end.\n",
    "        self.endList=0 #if 0, stay in the same list\n",
    "                         #if 1, go to the next list\n",
    "\n",
    "        self.initializeTags()\n",
    "        print self.page_tick\n",
    "        with open('userData.txt', 'w') as outfile: \n",
    "            #back up our data in case of emergencies\n",
    "            json.dump(self.Users, outfile)\n",
    "        with open('questionData.txt','w') as outfile:\n",
    "            json.dump(self.Questions, outfile)\n",
    "        self.scrapeLoop()\n",
    "\n",
    "                \n",
    "    def continueScrape(self, nQuestions=99999):\n",
    "        '''\n",
    "        If there is already a G2G analysis object, g2g, \n",
    "        with len(g2g.Users)>0 and len(g2g.Questions)>0,\n",
    "        pick up where you left off.\n",
    "        '''\n",
    "        self.page_tick=0 #start at the beginning of a tag\n",
    "        self.nQuestions=nQuestions\n",
    "\n",
    "        if len(self.tagDict)==0:\n",
    "            self.makeTagDict()\n",
    "            self.endList=1\n",
    "            self.newTagl()\n",
    "            self.runPage() \n",
    "        print self.home\n",
    "        self.scrapeLoop()\n",
    "                        \n",
    "    \n",
    "    def scrapeLoop(self):\n",
    "        '''\n",
    "        Turn the page; changing to a new tag if necessary.\n",
    "        '''\n",
    "        while self.flipped < self.nQuestions:\n",
    "            self.turnPage() # go to next page of questions.\n",
    "            self.newTag() # if I need to change tags, do so\n",
    "            self.runPage() # scrape all the questions on a page.\n",
    "            print self.page_tick\n",
    "            with open('userData.txt', 'w') as outfile: \n",
    "                #back up our data in case of emergencies\n",
    "                json.dump(self.Users, outfile)\n",
    "            with open('questionData.txt','w') as outfile:\n",
    "                json.dump(self.Questions, outfile)\n",
    "    \n",
    "    def initializeTags(self):\n",
    "        '''\n",
    "        create a tag dictionary from the launching page\n",
    "        '''\n",
    "        self.tagDict={}\n",
    "        self.tagPageIndex=0\n",
    "        self.makeTagDict()\n",
    "        self.endList=1\n",
    "        self.newTag()\n",
    "        #self.runPage()\n",
    "\n",
    "        \n",
    "    def turnPage(self):\n",
    "        '''\n",
    "        load the next page of questions within a tag\n",
    "        check whether I've reached the end of that tag\n",
    "        '''\n",
    "        nextPage=self.page_tick*50\n",
    "        pageUrl=self.home+'?start='+str(nextPage)\n",
    "        page = requests.get('http://www.newyorksocialdiary.com/party-pictures')\n",
    "        self.page=requests.get(pageUrl)\n",
    "        self.soup = BeautifulSoup(self.page.text, 'lxml')\n",
    "        \n",
    "        # Check to see there's still questions on this page.\n",
    "        pageTitle=self.soup.find('title').text\n",
    "        if pageTitle.find('No questions') > -1:\n",
    "            self.endList=1\n",
    "        div=self.soup.find('div', attrs={'class' : \"qa-part-q-list\"})\n",
    "        if div.text.find('No question') > -1: \n",
    "            #not sure which of these works consistently\n",
    "            self.endList=1\n",
    "\n",
    "            \n",
    "    def newTag(self):\n",
    "        '''\n",
    "        check whether I've reached the end of a tag\n",
    "        and move to the next, if necessary\n",
    "        or, signal I've reached the end of all the tags.\n",
    "        '''\n",
    "        if self.endList==1:\n",
    "            self.endList=0\n",
    "            self.page_tick=0\n",
    "\n",
    "            # move to the next tag.\n",
    "            tagDets=self.tagDict.popitem()\n",
    "            print tagDets\n",
    "            self.currentTag=tagDets[0]\n",
    "            self.currentTagCount=tagDets[1]\n",
    "            self.home=self.path+'tag/'+self.currentTag\n",
    "            self.page=requests.get(self.home)\n",
    "            self.soup=BeautifulSoup(self.page.text, 'lxml')\n",
    "            if len(self.tagDict)==0:\n",
    "                self.makeTagDict()\n",
    "        else:\n",
    "            pass    \n",
    "    \n",
    "    \n",
    "    def makeTagDict(self):\n",
    "        '''\n",
    "        Move to the next page of tags\n",
    "        read them all in, and\n",
    "        repopulate the dictionary with tags.\n",
    "        '''\n",
    "        pageStart=str(self.tagPageIndex*100)\n",
    "        tagPageUrl=self.path+'/tags?start='+pageStart\n",
    "        print tagPageUrl\n",
    "        tagPage = requests.get(tagPageUrl)\n",
    "        tagSoup = BeautifulSoup(tagPage.text, 'lxml')\n",
    "        for tag in tagSoup.findAll('td', attrs={'class':'qa-top-tags-count'}):\n",
    "            myCount=tag.text.split(' ')[0]\n",
    "            myCount=int(myCount.replace(',', ''))\n",
    "            tag2=tag.find_next()\n",
    "            link=tag2.find('a').get('href')\n",
    "            link=link[6:]\n",
    "            self.tagDict[link]=myCount\n",
    "        if self.tagPageIndex > 150: # we've almost certainly seen everything.\n",
    "            self.flipped=self.nQuestions+1 # jump to the end.\n",
    "        self.tagPageIndex+=1\n",
    "        \n",
    "        \n",
    "    def runPage(self):\n",
    "        '''\n",
    "        go through the list of questions on this page\n",
    "        and scrape each one in turn\n",
    "        also, check to see if this page is the same as the last one.\n",
    "        '''\n",
    "        self.page_tick+=1\n",
    "        if self.flipped < self.nQuestions:\n",
    "            for title in self.soup.findAll('div', attrs={'class': 'qa-q-item-title' }):\n",
    "                link=title.find('a').get('href')\n",
    "                qID=self.checkQuestion(link) #did I already ask it?\n",
    "                if qID:\n",
    "                    self.flipped+=1\n",
    "                    self.scrapeQuestion(link,qID)\n",
    "                    \n",
    "                    \n",
    "    def addUserAndText(self, uID,uText, uWhen):\n",
    "        '''\n",
    "        Update the info for a user\n",
    "        Initializing a new user if needed\n",
    "        '''\n",
    "        days = self.getDaysDiff(uWhen)\n",
    "        userID=uID.split('/user/')[1]\n",
    "        if not userID in self.Users:\n",
    "            self.addUser(userID)\n",
    "        else:\n",
    "            pass\n",
    "        textVal=self.get_text_sentiment(uText)\n",
    "        nSyllables=textstat.syllable_count(uText)\n",
    "        self.Users[userID]['textLens']+= [len(uText)]#another measure of their text\n",
    "        self.Users[userID]['textSent']+=[textVal]\n",
    "        self.Users[userID]['nSylls']+=[nSyllables]\n",
    "        self.Users[userID]['days']+=[days]\n",
    "        self.Users[userID]['questionIds']+=[self.qDict['question']]\n",
    "        self.qDict['count']+=1\n",
    "        self.qDict['tone']+=[textVal]\n",
    "        self.qDict['syllables']+=[nSyllables]\n",
    "        self.qDict['length']+=[len(uText)]\n",
    "        self.qDict['user']+=[userID]\n",
    "\n",
    "        \n",
    "    def addUser(self, userID):\n",
    "        '''\n",
    "        Add the G2G keys and values to the userDets dictionary\n",
    "        and intitialize the list values for the other keys\n",
    "        '''\n",
    "        userPath=self.path+'user/'+userID\n",
    "        userpage = requests.get(userPath)\n",
    "        userSoup=BeautifulSoup(userpage.text, 'lxml')\n",
    "        userDets={}\n",
    "        userDets['nPosts'] = userSoup.find('span', \n",
    "                                           attrs={'class' : \"qa-uf-user-q-posts\"}).text\n",
    "        userDets['nAnswers']=userSoup.find('span', \n",
    "                                           attrs={'class' : \"qa-uf-user-a-posts\"}).text\n",
    "        userDets['nComments'] = userSoup.find('span', \n",
    "                                              attrs={'class' : \"qa-uf-user-c-posts\"}).text\n",
    "        userDets['giveUp'] = userSoup.find('span', \n",
    "                                           attrs={'class' : \"qa-uf-user-upvotes\"}).text #gave\n",
    "        userDets['giveDown'] = userSoup.find('span', \n",
    "                                             attrs={'class' : \"qa-uf-user-downvotes\"}).text\n",
    "        userDets['getUp'] = userSoup.find('span', \n",
    "                                          attrs={'class' : \"qa-uf-user-upvoteds\"}).text #received\n",
    "        userDets['getDown'] = userSoup.find('span', \n",
    "                                            attrs={'class' : \"qa-uf-user-downvoteds\"}).text\n",
    "        userURL='https://www.wikitree.com/wiki/'+userID\n",
    "\n",
    "        userpage= requests.get(userURL)\n",
    "        userSoup=BeautifulSoup(userpage.text, 'lxml')\n",
    "        myThanks=0\n",
    "        for div in userSoup.findAll('div', attrs={'class':'SMALL'} ):\n",
    "            words=div.text.split(' ')\n",
    "            words=filter(lambda x: x!=' ', words)\n",
    "            words=filter(lambda x: x!='', words)\n",
    "            i=0\n",
    "            for x in words:\n",
    "                if 'contributions' in x:\n",
    "                    contributions=words[i-1]\n",
    "                if 'thank' in x:\n",
    "                    if myThanks==0:\n",
    "                        thanks=words[i-1]\n",
    "                        myThanks=1\n",
    "                if 'confirmed' in x:\n",
    "                    day=words[i+1]\n",
    "                    month=words[i+2]\n",
    "                    year=words[i+3]\n",
    "                i+=1    \n",
    "        \n",
    "        userDets['year']=int(year)\n",
    "        userDets['mon']=month\n",
    "        userDets['day']=day\n",
    "        userDets['thanks']=thanks\n",
    "        userDets['contributions']=contributions\n",
    "        userDets['textLens']=[]\n",
    "        userDets['textSent']=[]\n",
    "        userDets['nSylls']=[]\n",
    "        userDets['days']=[]\n",
    "        userDets['questionIds']=[]\n",
    "        \n",
    "        self.Users[userID]=userDets\n",
    "\n",
    "                    \n",
    "    def checkQuestion(self, link):\n",
    "        '''\n",
    "        make sure I haven't queried this question before\n",
    "        if not, \n",
    "        add the question to the dictionary of questions\n",
    "        '''\n",
    "        questNumber=link.split('/')[1]\n",
    "        \n",
    "        if not questNumber in self.Questions:\n",
    "            self.Questions[questNumber]={'path':self.path+link[3:]}\n",
    "            #questPage=requests.get(self.path+link[3:])\n",
    "            return questNumber\n",
    "        else:\n",
    "            return False\n",
    "                  \n",
    "            \n",
    "    def parseQuestion(self,qPage,qTitle):\n",
    "        '''\n",
    "        Go through the question, and the comments on the question\n",
    "        '''\n",
    "        #question= qPage.find('div', attrs={'class': 'qa-part-q-view' })\n",
    "        try:\n",
    "            question=qPage.find('div', attrs={'class': re.compile('qa-q-view\\shentry\\squestion')})\n",
    "            body=question.find('div', attrs={'class':'qa-q-view-main'})\n",
    "        except:\n",
    "            question=qPage.find('div', attrs={'class':re.compile('qa-q-view\\sqa-q-closed\\shentry\\squestion')})\n",
    "            body=question.find('div', attrs={'class':'qa-q-view-main'})\n",
    "        try:\n",
    "            questionText=body.find('div', attrs={'class': 'entry-content' }).text\n",
    "        except:\n",
    "            questionText=''\n",
    "        try:\n",
    "            questionText=qTitle+' '+questionText\n",
    "            qID=body.find('a', attrs={'class': 'qa-user-link' }).get('href')\n",
    "            whenQ=question.find('span', attrs={'class':re.compile('qa-q-view-when-data')}).text\n",
    "            self.addUserAndText(qID,questionText,whenQ)\n",
    "        except:\n",
    "            pass\n",
    "        comments=question.find('div', attrs={'class':'qa-q-view-c-list'}) \n",
    "\n",
    "        for comment in comments.findAll('div', attrs={'class' : re.compile('qa-c-list-item\\shentry\\scomment')}):\n",
    "            try:\n",
    "                self.parseComments(comment)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    def parseAnswer(self,answer):\n",
    "        answerText=answer.find('div', attrs={'class':'qa-a-item-content'}).text\n",
    "        #print answerText\n",
    "        answerID=answer.find('a', attrs={'class':'qa-user-link'}).get('href')\n",
    "        whenAnswer=answer.find('span', attrs={'class':'qa-a-item-when-data'}).text\n",
    "        self.addUserAndText(answerID,answerText,whenAnswer)\n",
    "        comments = answer.find('div', attrs={'class':'qa-a-item-c-list'})\n",
    "        for comment in comments.findAll('div', attrs={'class': re.compile('qa-c-list-item\\shentry\\scomment')}):\n",
    "            self.parseComments(comment)\n",
    "            \n",
    "            \n",
    "    def extractQuestionText(self,qPage,qTitle):\n",
    "        '''\n",
    "        Check to see how many up/downvotes a question has\n",
    "        Save it as an either positive, negative, or neutral corpus.\n",
    "        '''\n",
    "        #question= qPage.find('div', attrs={'class': 'qa-part-q-view' })\n",
    "        try:\n",
    "            question=qPage.find('div', attrs={'class': re.compile('qa-q-view\\shentry\\squestion')})\n",
    "            body=question.find('div', attrs={'class':'qa-q-view-main'})\n",
    "        except:\n",
    "            question=qPage.find('div', attrs={'class':re.compile('qa-q-view\\sqa-q-closed\\shentry\\squestion')})\n",
    "            body=question.find('div', attrs={'class':'qa-q-view-main'})\n",
    "        try:\n",
    "            questionText=body.find('div', attrs={'class': 'entry-content' }).text\n",
    "        except:\n",
    "            questionText=''\n",
    "        try:\n",
    "            questionText=qTitle+' '+questionText\n",
    "            print question\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    def extractAnswerText(self,answer):\n",
    "        answerText=answer.find('div', attrs={'class':'qa-a-item-content'}).text\n",
    "        #print answerText\n",
    "        answerID=answer.find('a', attrs={'class':'qa-user-link'}).get('href')\n",
    "        whenAnswer=answer.find('span', attrs={'class':'qa-a-item-when-data'}).text\n",
    "        self.addUserAndText(answerID,answerText,whenAnswer)\n",
    "        comments = answer.find('div', attrs={'class':'qa-a-item-c-list'})\n",
    "        for comment in comments.findAll('div', attrs={'class': re.compile('qa-c-list-item\\shentry\\scomment')}):\n",
    "            self.parseComments(comment)\n",
    "\n",
    "            \n",
    "    def parseComments(self, comment):\n",
    "        cText=comment.find('div', attrs={'class':'entry-content'}).text\n",
    "        #print cText\n",
    "        try:\n",
    "            cID=comment.find('a',  attrs={'class': 'qa-user-link'} ).get('href')\n",
    "        except:\n",
    "            pass\n",
    "        cWhen=comment.find('span', attrs={'class' : 'qa-c-item-when-data'}).text\n",
    "        self.addUserAndText(cID,cText,cWhen)\n",
    "        \n",
    "\n",
    "    def scrapeQuestion(self,link, qId):\n",
    "        '''\n",
    "        Go through the page\n",
    "        1.0: the question (only one)\n",
    "        1.1: the comments on the question (0 to many) \n",
    "        2.0: the answers (0 to many)\n",
    "        2.1: the comments on each answer in turn (0 to many)\n",
    "        '''\n",
    "        #qDict will hold the record of the question statistics\n",
    "        #print 'am I scraping'\n",
    "        self.qDict={'question':qId, 'count':0,'tone':[], 'syllables':[],'length':[], 'user':[]}\n",
    "        #print self.qDict\n",
    "        link=self.path+link[3:]\n",
    "        page = requests.get(link)\n",
    "        qPage=BeautifulSoup(page.text, 'lxml')\n",
    "        qTitle=qPage.find('title').text\n",
    "        qTitle=qTitle[:(qTitle.find(' - WikiTree G2G'))]\n",
    "        #print ' '\n",
    "        #1.0\n",
    "        self.parseQuestion(qPage, qTitle)\n",
    "        for answer in qPage.findAll('div', attrs={'class': re.compile('qa-a-list-item\\shentry\\sanswer')}):\n",
    "            try:\n",
    "                self.parseAnswer(answer)\n",
    "            except:\n",
    "                pass\n",
    "        try:\n",
    "            bestAnswer= qPage.find('div', attrs={'class': re.compile('qa-a-list-item\\shentry\\sanswer\\sanswer-selected\\sqa-a-list-item-selected')})\n",
    "            self.parseAnswer(bestAnswer)\n",
    "        except:\n",
    "            pass\n",
    "        self.Questions[qId]['stats']=self.qDict\n",
    "                \n",
    "                \n",
    "    def clean_text(self, text):\n",
    "        '''\n",
    "        Utility function to clean text by removing links, special characters\n",
    "        using simple regex statements.\n",
    "        '''\n",
    "        cleanText=' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \n",
    "                                  \" \", text).split())\n",
    "        return cleanText\n",
    "        \n",
    "        \n",
    "    def get_text_sentiment(self, text):\n",
    "        '''\n",
    "        Utility function to classify sentiment of passed tweet\n",
    "        using textblob's sentiment method\n",
    "        '''\n",
    "        text=self.clean_text(text)\n",
    "        analysis = TextBlob(text)\n",
    "        return analysis.sentiment.polarity\n",
    "\n",
    "    def getDaysDiff(self, uWhen):\n",
    "        '''\n",
    "        Calculate how long ago the question was.\n",
    "        '''\n",
    "        uWhen=uWhen.strip()\n",
    "        if uWhen.find('minute')>-1:\n",
    "            days=0\n",
    "        elif uWhen.find('hour')>-1:\n",
    "            days=0\n",
    "        elif uWhen.find('day')>-1:\n",
    "            days = uWhen[0]\n",
    "        elif len(uWhen.split(' '))<3:\n",
    "            uWhen=uWhen+' 2018'\n",
    "            then=datetime.strptime(uWhen, '%b %d %Y')\n",
    "            dateDiff=(datetime.now()-then)\n",
    "            days=dateDiff.days           \n",
    "        else:\n",
    "            uWhen=uWhen.replace(',','')\n",
    "            then=datetime.strptime(uWhen, '%b %d %Y')\n",
    "            dateDiff=(datetime.now()-then)\n",
    "            days=dateDiff.days\n",
    "        return days\n",
    "    \n",
    "    \n",
    "    def reloadSession(self,userpath='userData.txt', questionpath='questionData.txt', nQuestions=99999):\n",
    "        '''\n",
    "        initialise a new session, \n",
    "        then import a bunch of saved questions and users\n",
    "        '''\n",
    "        self.nQuestions=nQuestions\n",
    "        self.initializeTags()\n",
    "        with open(questionpath) as json_data:\n",
    "            self.Questions = json.load(json_data)\n",
    "        \n",
    "        with open(userpath) as json_data:\n",
    "            self.Users = json.load(json_data)\n",
    "            \n",
    "        \n",
    "    def getScoredText(self):\n",
    "        \n",
    "        pass\n",
    "\n",
    "        \n",
    "    def importG2G(self, otherG2G):\n",
    "        '''\n",
    "        copy over the dicts and things from an old G2G analysis, in order to keep going\n",
    "        only works on tag analysis so far\n",
    "        '''\n",
    "        self.Questions=otherG2G.Questions\n",
    "        self.Users=otherG2G.Users\n",
    "        self.home=otherG2G.home\n",
    "        self.path=otherG2G.path\n",
    "        self.tagDict=otherG2G.tagDict\n",
    "        self.topQuestion=otherG2G.topQuestion\n",
    "        self.currentTagCount=otherG2G.currentTagCount\n",
    "        self.soup=otherG2G.soup\n",
    "        self.page=otherG2G.page\n",
    "        self.tagPageIndex=otherG2G.tagPageIndex\n",
    "        self.endList=otherG2G.endList\n",
    "    \n",
    "g2g=G2GAnalysis()\n",
    "#g2g.runScrape()\n",
    "g2g.reloadSession(userpath='userData_trans.txt', questionpath='questionData.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import random\n",
    "\n",
    "def extractQuestionText(qPage,qTitle, viewCount, running_threshold):\n",
    "    '''\n",
    "    Check to see how many up/downvotes a question has\n",
    "    Save it as an either positive, negative, or neutral corpus.\n",
    "    '''\n",
    "    #question= qPage.find('div', attrs={'class': 'qa-part-q-view' }\n",
    "    try:\n",
    "        question = qPage.find('div', attrs={'class': re.compile('qa-q-view\\shentry\\squestion')})\n",
    "        body = question.find('div', attrs={'class':'qa-q-view-main'})\n",
    "    except:\n",
    "        question = qPage.find('div', attrs={'class':re.compile('qa-q-view\\sqa-q-closed\\shentry\\squestion')})\n",
    "        body = question.find('div', attrs={'class':'qa-q-view-main'})\n",
    "    try:\n",
    "        text=body.find('div', attrs={'class': 'entry-content' }).text\n",
    "    except:\n",
    "        text = ''\n",
    "    try:\n",
    "        text = qTitle+' '+text\n",
    "    except:\n",
    "        return 0\n",
    "    votes = question.find('span', attrs={'class': 'qa-netvote-count-data'}).text\n",
    "    saved=saveTraining(votes, viewCount, running_threshold, text)\n",
    "    return saved\n",
    "    \n",
    "    \n",
    "def extractAnswerText(answer, viewCount, running_threshold):\n",
    "    votes = answer.find('span', attrs={'class': 'qa-netvote-count-data'}).text\n",
    "    text=answer.find('div', attrs={'class':'qa-a-item-content'}).text\n",
    "    votes = int(votes)\n",
    "    saved=saveTraining(votes, viewCount, running_threshold, text)\n",
    "    return saved\n",
    "    \n",
    "    \n",
    "def votes_to_int(votes):\n",
    "    '''\n",
    "    screw them with their decorative n-dash\n",
    "    '''\n",
    "    if votes[0]==u'\\u2013':\n",
    "        votes = votes[1:]\n",
    "        votes=0-int(str(votes))\n",
    "    else:\n",
    "        votes=int(str(votes))\n",
    "    return votes\n",
    "\n",
    "    \n",
    "def saveTraining(votes, viewCount, running_threshold, text):\n",
    "    votes = votes_to_int(votes)\n",
    "    vote_rat = votes/(np.log(viewCount+10))\n",
    "    if vote_rat > running_threshold:\n",
    "        outname = './training/pos/outie_'+str(votes)+'_'+str(random.randrange(10000000))+'.txt'\n",
    "        f=open(outname, 'w+')\n",
    "        f.write(text.encode('utf-8').strip())\n",
    "        return 1\n",
    "    elif vote_rat <0:\n",
    "        outname = './training/neg/outie_'+str(votes)+'_'+str(random.randrange(10000000))+'.txt'\n",
    "        f=open(outname, 'w+')\n",
    "        f.write(text.encode('utf-8').strip())\n",
    "        return 0\n",
    "    else:\n",
    "        if (random.random() < 0.1) and viewCount > 10 and len(text) > 30:\n",
    "            outname = './training/neut/outie_'+str(votes)+'_'+str(random.randrange(10000000))+'.txt'\n",
    "            f=open(outname, 'w+')\n",
    "            f.write(text.encode('utf-8').strip())\n",
    "        return 0\n",
    "\n",
    "def updateRunner(current_good, target_ratio, i, running_threshold):\n",
    "    if ((current_good*1.)/i)>target_ratio:\n",
    "        running_threshold=running_threshold*(1.+1./np.sqrt(i+1.))\n",
    "    else:\n",
    "        running_threshold=running_threshold*(1.-1./np.sqrt(i+1.))\n",
    "    return running_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-367b0b2f9981>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtarget_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.05\u001b[0m \u001b[0;31m# proportion saved as 'high ranks'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_vals\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mg2g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQuestions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_vals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mqPage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/requests/api.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/requests/api.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    500\u001b[0m         }\n\u001b[1;32m    501\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/requests/models.pyc\u001b[0m in \u001b[0;36mcontent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    823\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONTENT_CHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content_consumed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/requests/models.pyc\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    745\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/urllib3/response.pyc\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    430\u001b[0m         \"\"\"\n\u001b[1;32m    431\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_chunked_reads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/urllib3/response.pyc\u001b[0m in \u001b[0;36mread_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    595\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m                 \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m                 decoded = self._decode(chunk, decode_content=decode_content,\n\u001b[1;32m    599\u001b[0m                                        flush_decoder=False)\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/urllib3/response.pyc\u001b[0m in \u001b[0;36m_handle_chunk\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# amt > self.chunk_left\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mreturned_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Toss the CRLF at the end of the chunk.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreturned_chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36m_safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAXAMOUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0;31m# fragmentation issues on many platforms.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/urllib3/contrib/pyopenssl.pyc\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSysCallError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Unexpected EOF'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/dist-packages/OpenSSL/SSL.pyc\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, bufsiz, flags)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL_peek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsiz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsiz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_ssl_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i=0\n",
    "running_threshold = 1. # threshold goodness\n",
    "current_good = 0.1\n",
    "target_ratio = 0.05 # proportion saved as 'high ranks'\n",
    "for q_id, q_vals in g2g.Questions.iteritems():\n",
    "    page = requests.get(q_vals['path'])\n",
    "    try:\n",
    "        qPage=BeautifulSoup(page.text, 'lxml')\n",
    "        qTitle=qPage.find('title').text\n",
    "        qTitle=qTitle[:(qTitle.find(' - WikiTree G2G'))]\n",
    "        viewCount=qPage.find('span', attrs={'class': re.compile('qa-view-count-data')}).text\n",
    "        viewCount=int(viewCount.replace(',',''))\n",
    "        current_good+=extractQuestionText(qPage, qTitle, viewCount, running_threshold)\n",
    "        i+=1\n",
    "        running_threshold=updateRunner(current_good, target_ratio, i, running_threshold)\n",
    "    except:\n",
    "        pass\n",
    "    for answer in qPage.findAll('div', attrs={'class': re.compile('qa-a-list-item\\shentry\\sanswer')}):\n",
    "        try:\n",
    "            current_good+=extractAnswerText(answer, viewCount, running_threshold)\n",
    "            i+=1\n",
    "            running_threshold=updateRunner(current_good, target_ratio, i, running_threshold)\n",
    "        except:\n",
    "            pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionText=\"I adopted Hawkins-1533 and it is same as Hawkins-332 Please look and merge , Thanks After researching I went back on the profile of Elizabeth, Hawkins-332 and wasn\\'t paying attention to them being to and adopted and they was one already with the information Hawkins-332, but I didn\\'t add nothing just need them merged,  sorry and Thank\\'s I don't know how to take my name off manage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am the 22nd grt nephew of King John\n",
      "101\n",
      "I am the 22nd grt nephew of King John John Plantagenet Signed the Magna Carta. He is my 22nd great Uncle\n",
      "–3\n",
      "here\n",
      "–3\n",
      "–3\n",
      "here\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "running_threshold = 1. # threshold goodness\n",
    "current_good = 0.1\n",
    "target_ratio = 0.05 # proportion saved as 'high ranks'\n",
    "page = requests.get('https://www.wikitree.com/g2g/598684/i-am-the-22nd-grt-nephew-of-king-john')\n",
    "try:\n",
    "    qPage=BeautifulSoup(page.text, 'lxml')\n",
    "    qTitle=qPage.find('title').text\n",
    "    qTitle=qTitle[:(qTitle.find(' - WikiTree G2G'))]\n",
    "    viewCount=qPage.find('span', attrs={'class': re.compile('qa-view-count-data')}).text\n",
    "    viewCount=int(viewCount.replace(',',''))\n",
    "    print qTitle\n",
    "    print viewCount\n",
    "    current_good+=extractQuestionText(qPage, qTitle, viewCount, running_threshold)\n",
    "    i+=1\n",
    "    running_threshold=updateRunner(current_good, target_ratio, i, running_threshold)\n",
    "except:\n",
    "    pass\n",
    "for answer in qPage.findAll('div', attrs={'class': re.compile('qa-a-list-item\\shentry\\sanswer')}):\n",
    "    try:\n",
    "        current_good+=extractAnswerText(answer, viewCount, running_threshold)\n",
    "        i+=1\n",
    "        running_threshold=updateRunner(current_good, target_ratio, i, running_threshold)\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–3\n"
     ]
    }
   ],
   "source": [
    "print '–3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./training/pos/outie316901.txt\n",
      "I adopted Hawkins-1533 and it is same as Hawkins-332 Please look and merge , Thanks After researching I went back on the profile of Elizabeth, Hawkins-332 and wasn't paying attention to them being to and adopted and they was one already with the information Hawkins-332, but I didn't add nothing just need them merged,  sorry and Thank's I don't know how to take my name off manage\n"
     ]
    }
   ],
   "source": [
    "outname = './training/pos/outie316901.txt'\n",
    "print outname\n",
    "print questionText.encode('utf-8').strip()\n",
    "f=open(outname, 'w+')\n",
    "f.write(questionText.encode('utf-8').strip())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(g2g.Questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gibberish\n"
     ]
    }
   ],
   "source": [
    "if 1>2 and 3<4 and 'apple'=='apple':\n",
    "    print 'true'\n",
    "else:\n",
    "    print 'gibberish'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "from textstat.textstat import textstat\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "lunk='https://www.wikitree.com/'\n",
    "link='/g2g/556814/public-record-office-announces-digitizing-records-seemed'\n",
    "\n",
    "thing=G2GAnalysis()\n",
    "qId=thing.checkQuestion(link)\n",
    "thing.scrapeQuestion(link, qId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page=requests.get('https://www.wikitree.com/g2g/556814/public-record-office-announces-digitizing-records-seemed').text\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
